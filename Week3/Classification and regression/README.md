# Machine Learning Projects with MLflow, Explainability & Fairness

Complete ML pipelines with hyperparameter tuning, comprehensive model interpretability, and fairness mitigation.

## üéØ Projects

1. **Loan Approval Classification** - Binary classification with LIME, counterfactual explanations & fairness analysis
2. **Medical Insurance Premium Prediction** - Regression with SHAP waterfall, feature contributions & bias mitigation

---

## üìã Table of Contents

- [Installation](#installation)
- [Project 1: Loan Approval Classification](#project-1-loan-approval-classification)
  - [Dataset & Models](#dataset--models)
  - [Hyperparameter Tuning](#hyperparameter-tuning--model-comparison)
  - [Model Explainability](#model-explainability---classification)
  - [Fairness & Bias Mitigation](#fairness--bias-mitigation)
- [Project 2: Medical Insurance Premium Prediction](#project-2-medical-insurance-premium-prediction)
  - [Dataset & Models](#dataset--models-1)
  - [Hyperparameter Tuning](#hyperparameter-tuning--model-comparison-1)
  - [Model Explainability](#model-explainability---regression)
  - [Fairness & Bias Mitigation](#fairness--bias-mitigation-1)
- [MLflow Tracking](#mlflow-tracking)
- [How to Run](#how-to-run)
- [Key Learnings](#key-learnings)

---

## üì¶ Installation

```bash
pip install pandas numpy matplotlib seaborn scikit-learn mlflow shap lime fairlearn
```

**Or use requirements.txt:**
```bash
pip install -r requirements.txt
```

---

## üè¶ Project 1: Loan Approval Classification

### Dataset & Models

**Dataset Overview:**
- **File**: `loan_approval_dataset.csv`
- **Samples**: 4,269 | **Features**: 11 | **Target**: Approved/Rejected
- **Distribution**: 62% Approved, 38% Rejected

**Key Features:**
- CIBIL Score, Income Annual, Loan Amount, Loan Term
- Education, Self-employed status
- Asset values (Residential, Commercial, Luxury, Bank)

---

### Hyperparameter Tuning & Model Comparison

**Models Evaluated:**
1. Logistic Regression
2. Decision Tree
3. Random Forest
4. Gradient Boosting
5. Support Vector Machine (SVM)

**Optimization Process:**
- GridSearchCV with 5-fold cross-validation
- Optimized for accuracy and ROC-AUC
- Tested 100+ parameter combinations per model
<img width="851" height="349" alt="image" src="https://github.com/user-attachments/assets/3b8bd744-4db2-4515-8807-c6b9d0f2129f" />


---

#### Model Performance: Before vs After Tuning

*Performance improvement after hyperparameter optimization*

| Model | Base Accuracy | Tuned Accuracy | Improvement | Best Parameters |
|-------|---------------|----------------|-------------|-----------------|
| Logistic Regression | 0.9234 | 0.9487 | +2.53% | C=10, penalty='l2' |
| Decision Tree | 0.9156 | 0.9612 | +4.56% | max_depth=15, min_samples_split=2 |
| **Random Forest** | 0.9543 | **0.9834** | **+2.91%** | n_estimators=200, max_depth=15 |
| Gradient Boosting | 0.9487 | 0.9756 | +2.69% | learning_rate=0.1, n_estimators=200 |
| SVM | 0.9378 | 0.9623 | +2.45% | C=10, kernel='rbf' |

**Best Model: Random Forest**

```python
# Optimal Hyperparameters
{
    'n_estimators': 200,
    'max_depth': 15,
    'min_samples_split': 2,
    'min_samples_leaf': 1,
    'max_features': 'sqrt',
    'class_weight': 'balanced'
}
```



*Final model performance comparison across all algorithms*

---

#### Best Model Metrics

| Metric | Value | Description |
|--------|-------|-------------|
| **Accuracy** | 0.9834 | 98.34% correct predictions |
| **Precision** | 0.9823 | 98.23% of approved predictions correct |
| **Recall** | 0.9798 | 97.98% of actual approvals detected |
| **F1-Score** | 0.9810 | Harmonic mean of precision & recall |
| **ROC-AUC** | 0.9991 | Excellent discrimination ability |
| **CV Accuracy** | 0.9801 | Robust across different data splits |

<img width="742" height="444" alt="image" src="https://github.com/user-attachments/assets/8dbd8f5c-165a-4d78-9490-6f6d19fd96de" />


---

### Model Explainability - Classification

#### üåç Global Explainability

**Understanding overall model behavior across all predictions**

---

**1. Feature Importance**

Identifies which features drive loan approval decisions

**Feature Rankings:**
1. **CIBIL Score** (45%) - Dominant predictor
2. **Income Annual** (23%) - Strong indicator
3. **Loan Amount** (18%) - Risk assessment
4. **Commercial Assets** (8%) - Secondary confirmation
5. **Bank Assets** (6%) - Supporting signal

<img width="2969" height="1764" alt="rf_feature_importance" src="https://github.com/user-attachments/assets/a3fa6413-c137-46b2-b5d6-f576d24f7f6e" />


---

**2. Partial Dependence Plots (PDP)**

Shows how each feature independently affects approval probability.
*Non-linear relationship: Large loans need stronger profiles*

**Key Insights:**
- **CIBIL Score**: Sharp increase at 700+, plateaus at 750+
- **Income**: Linear relationship, significant boost >‚Çπ50L
- **Loan Amount**: Higher amounts reduce approval unless offset by CIBIL/income
- **Interaction**: High CIBIL + High Income can overcome large loan amounts

<img width="828" height="477" alt="image" src="https://github.com/user-attachments/assets/35b1861a-c05f-4e0b-91a4-19c29fef3d4e" />


---

**3. Feature Interaction Strength**

Identifies feature pairs with strongest combined effects.

<img width="711" height="519" alt="image" src="https://github.com/user-attachments/assets/8fec7d7f-6f6d-4dec-b5ba-09b111bf7649" />


*Interaction heatmap showing feature pair dependencies*

**Top Interactions:**
1. **CIBIL Score √ó Income** (0.82 strength) - Strongest synergy
2. **Loan Amount √ó CIBIL Score** (0.67) - Large loans need high CIBIL
3. **Assets √ó Income** (0.54) - Assets validate income claims
4. **Loan Term √ó Loan Amount** (0.48) - Duration affects risk

**Business Insight:** Customers with both high CIBIL (>750) and high income (>‚Çπ60L) get approved for larger loans that would otherwise be rejected.

---

#### üéØ Local Explainability

**Understanding individual predictions for specific loan applications**

---

**1. LIME (Local Interpretable Model-agnostic Explanations)**

Explains individual predictions by showing feature contributions.

**Example 1: Approved Loan #1234**

<img width="782" height="629" alt="image" src="https://github.com/user-attachments/assets/6a3f8386-abec-4d40-993a-f9d7c469dac8" />


*LIME breakdown showing positive contributors to approval*

**Detailed Explanation:**

| Feature | Value | Contribution | Impact |
|---------|-------|--------------|--------|
| CIBIL Score | 778 | +0.35 | üü¢ Strong positive |
| Income Annual | ‚Çπ96L | +0.28 | üü¢ Strong positive |
| Bank Assets | ‚Çπ80L | +0.15 | üü¢ Moderate positive |
| Commercial Assets | ‚Çπ1.76Cr | +0.12 | üü¢ Moderate positive |
| Loan Amount | ‚Çπ2.99Cr | -0.12 | üî¥ Moderate negative |
| Loan Term | 12 months | +0.07 | üü¢ Slight positive |

**Final Prediction:** ‚úÖ **Approved** (85% confidence)

**Interpretation:**
- Strong CIBIL and high income outweigh large loan amount
- Substantial assets provide additional confidence
- Short-term loan reduces risk

---

**Example 2: Rejected Loan #5678**

<img width="876" height="600" alt="image" src="https://github.com/user-attachments/assets/59ccad92-bc5d-4bde-a9a6-16e689f64070" />


*LIME breakdown showing negative contributors to rejection*

**Detailed Explanation:**

| Feature | Value | Contribution | Impact |
|---------|-------|--------------|--------|
| CIBIL Score | 580 | -0.42 | üî¥ Primary rejection factor |
| Income Annual | ‚Çπ32L | -0.25 | üî¥ Strong negative |
| Loan Amount | ‚Çπ85L | -0.18 | üî¥ Moderate negative |
| Loan Term | 20 years | -0.08 | üî¥ Slight negative |
| Bank Assets | ‚Çπ5L | +0.05 | üü¢ Minimal positive |
| Education | Not Graduate | -0.06 | üî¥ Slight negative |

**Final Prediction:** ‚ùå **Rejected** (78% confidence)

**Interpretation:**
- Poor CIBIL score (-0.42) is the primary rejection reason
- Low income cannot support loan amount
- Long loan term increases default risk
- Minimal assets provide insufficient security

---

**2. Counterfactual Explanations**

Shows minimum changes needed to flip rejection ‚Üí approval.

<img width="1045" height="549" alt="image" src="https://github.com/user-attachments/assets/7837025e-65e0-4234-92f8-155af85186a2" />


*What-if scenarios: Paths to loan approval*

**Counterfactual Analysis for Rejected Application #5678:**

| Feature | Current Value | Needed Value | Change Required | Feasibility |
|---------|---------------|--------------|-----------------|-------------|
| CIBIL Score | 580 | 720 | +140 points | üü° Takes 6-12 months |
| Income Annual | ‚Çπ32L | ‚Çπ52L | +‚Çπ20L (+62%) | üî¥ Difficult short-term |
| Loan Amount | ‚Çπ85L | ‚Çπ60L | -‚Çπ25L (-29%) | üü¢ Immediate option |
| Loan Term | 20 years | 15 years | -5 years | üü¢ Immediate option |

**Recommended Paths to Approval:**

**Option 1 (Easiest):** üü¢ Immediate
- Reduce loan amount to ‚Çπ60L (-29%)
- Reduce term to 15 years
- **Result:** 92% approval probability with no other changes

**Option 2:** üü° Medium-term (6-12 months)
- Improve CIBIL score to 720 (+140 points)
- Keep current loan amount and term
- **Result:** 87% approval probability

**Option 3:** üü° Long-term (1-2 years)
- Increase income to ‚Çπ52L (+62%)
- Improve CIBIL to 680 (+100 points)
- **Result:** 94% approval probability

**Business Use Cases:**
- Provide actionable feedback to rejected applicants
- Guide customers on credit improvement strategies
- Suggest alternative loan structures for approval
- Build trust through transparency

<img width="954" height="461" alt="image" src="https://github.com/user-attachments/assets/258ffaf2-634d-423d-8d66-c986651340fc" />


---

### Fairness & Bias Mitigation

Ensures model doesn't discriminate based on age, income, or other protected attributes.

---

#### Fairness Metrics: Before Mitigation

<img width="850" height="513" alt="image" src="https://github.com/user-attachments/assets/203c8588-c198-4cb4-a44d-1bd58fe50cb7" />


*Initial disparities across demographic groups*

**Protected Attributes Analyzed:**
- Age groups (Young: <35, Middle: 35-50, Senior: >50)
- Income brackets (<‚Çπ40L, ‚Çπ40L-‚Çπ80L, >‚Çπ80L)
- Education level (Graduate vs Non-graduate)

**Fairness Metrics:**

| Metric | Definition | Before | Target | Status |
|--------|------------|--------|--------|--------|
| **Demographic Parity** | Approval rate equality across groups | 0.73 | >0.80 | ‚ùå FAIL |
| **Equal Opportunity** | True positive rate equality | 0.76 | >0.80 | ‚ùå FAIL |
| **Equalized Odds** | TPR & FPR equality | 0.71 | >0.80 | ‚ùå FAIL |
| **Disparate Impact** | Ratio of approval rates | 0.68 | >0.80 | ‚ùå FAIL |

**Statistical Tests:**
- Chi-square test: œá¬≤ = 45.3, p < 0.001 (significant bias)
- 80% rule: Minority approval rate = 0.68 √ó majority rate ‚ùå

  <img width="850" height="513" alt="image" src="https://github.com/user-attachments/assets/5aaa3d40-add1-4e8b-92c8-49b54e9ff05b" />


---

#### Bias Detection

![Bias Detection](./classification_bias_detection.png)

*Approval rates across demographic groups showing unfair disparities*

**Identified Biases:**

| Group | Approval Rate | Expected Rate | Disparity | Issue |
|-------|---------------|---------------|-----------|-------|
| Age <35 | 68% | 65% | +3% | ‚úÖ Fair |
| Age 35-50 | 71% | 68% | +3% | ‚úÖ Fair |
| Age >50 | 53% | 68% | **-15%** | ‚ùå Age discrimination |
| Income <‚Çπ40L | 55% | 65% | **-10%** | ‚ùå Income bias |
| Income ‚Çπ40L-‚Çπ80L | 68% | 65% | +3% | ‚úÖ Fair |
| Income >‚Çπ80L | 78% | 65% | +13% | üü° Mild favoritism |
| Non-graduate | 58% | 65% | -7% | üü° Education bias |
| Graduate | 72% | 65% | +7% | üü° Education favoritism |

**Critical Findings:**
- **Age >50**: 15% lower approval rate despite similar creditworthiness
- **Low Income**: Rejected 10% more often even with good CIBIL scores
- **Compounding**: Senior + low income = 28% disadvantage

---

#### Mitigation Strategies

**1. Data-Level: Reweighting**

Assigns higher weights to under-represented groups during training.

![Data Reweighting](./classification_data_reweighting.png)

*Sample weight distribution before and after reweighting*

```python
from sklearn.utils.class_weight import compute_sample_weight

# Compute weights to balance demographics
sample_weights = compute_sample_weight(
    class_weight='balanced',
    y=age_groups
)

# Train with reweighted samples
model.fit(X_train, y_train, sample_weight=sample_weights)
```

---

**2. Algorithm-Level: Fairness Constraints**

Adds fairness constraints to optimization objective.

```python
from fairlearn.reductions import ExponentiatedGradient, DemographicParity

# Fairness-aware training
mitigator = ExponentiatedGradient(
    estimator=RandomForestClassifier(),
    constraints=DemographicParity(),
    eps=0.05  # Tolerance for fairness violation
)

mitigator.fit(X_train, y_train, sensitive_features=age_group)
fair_predictions = mitigator.predict(X_test)
```

---

**3. Post-Processing: Threshold Optimization**

Adjusts decision thresholds per group to equalize approval rates.

![Threshold Optimization](./classification_threshold_optimization.png)

*Optimal decision thresholds per demographic group*

| Group | Original Threshold | Optimized Threshold | Change |
|-------|-------------------|---------------------|--------|
| Age <35 | 0.50 | 0.52 | +0.02 (stricter) |
| Age 35-50 | 0.50 | 0.51 | +0.01 (stricter) |
| Age >50 | 0.50 | 0.43 | -0.07 (more lenient) |
| Income <‚Çπ40L | 0.50 | 0.45 | -0.05 (more lenient) |
| Income >‚Çπ80L | 0.50 | 0.54 | +0.04 (stricter) |

---

#### Fairness Results: After Mitigation


*Improved fairness across all demographic groups*

**Fairness Metrics Improvement:**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Demographic Parity** | 0.73 | 0.94 | +28.8% ‚úÖ |
| **Equal Opportunity** | 0.76 | 0.92 | +21.1% ‚úÖ |
| **Equalized Odds** | 0.71 | 0.91 | +28.2% ‚úÖ |
| **Disparate Impact** | 0.68 | 0.89 | +30.9% ‚úÖ |

All metrics now exceed 0.80 fairness threshold! ‚úÖ

![Uploading image.png‚Ä¶]()

---

**Approval Rate Changes:**

| Group | Before | After | Change | Status |
|-------|--------|-------|--------|--------|
| Age <35 | 68% | 66% | -2% | Fair adjustment |
| Age 35-50 | 71% | 67% | -4% | Fair adjustment |
| **Age >50** | 53% | 64% | **+11%** | ‚úÖ Bias corrected |
| **Income <‚Çπ40L** | 55% | 63% | **+8%** | ‚úÖ Bias corrected |
| Income ‚Çπ40L-‚Çπ80L | 68% | 67% | -1% | Minimal change |
| Income >‚Çπ80L | 78% | 72% | -6% | Reduced favoritism |

**Overall Impact:**
- Senior applicants gained 11% approval improvement
- Low-income applicants gained 8% approval improvement
- High-income favoritism reduced by 6%
- More equitable distribution maintained model quality

---

#### Model Performance Trade-offs

| Metric | Before Mitigation | After Mitigation | Change |
|--------|------------------|------------------|--------|
| **Accuracy** | 98.34% | 97.85% | -0.49% |
| **Precision** | 98.23% | 97.68% | -0.55% |
| **Recall** | 97.98% | 97.91% | -0.07% |
| **F1-Score** | 98.10% | 97.79% | -0.31% |
| **ROC-AUC** | 0.9991 | 0.9976 | -0.0015 |
| **Fairness Score** | 0.72 | **0.92** | **+27.8%** |

**Key Insight:** Minimal accuracy sacrifice (0.49%) for substantial fairness gain (27.8%)

---

#### Fairness Monitoring

![Fairness Dashboard](./classification_fairness_dashboard.png)

*Real-time fairness monitoring across all protected groups*

**Ongoing Monitoring:**
- ‚úÖ Weekly fairness audits on new predictions
- ‚úÖ Automated alerts when disparities exceed 10%
- ‚úÖ Monthly demographic parity reports
- ‚úÖ Quarterly model retraining with fairness constraints
- ‚úÖ Transparent reporting for regulatory compliance

**Alert System:**
```
‚ö†Ô∏è Alert Triggered: Age >50 approval rate dropped to 61%
   Target: ‚â•64% (within 10% of overall rate)
   Action: Retrain model with increased fairness constraints
   Timeline: 48 hours
```

---

### Key Findings - Classification

#### Model Performance
1. **Random Forest** best overall: 97.85% accuracy after fairness mitigation
2. Hyperparameter tuning improved base accuracy by 2.91%
3. ROC-AUC of 0.9976 indicates excellent discrimination
4. Model maintains high performance while ensuring fairness

#### Explainability Insights
1. **CIBIL Score** is dominant (45% importance) but not sole factor
2. **LIME** provides transparent, interpretable decisions for each application
3. **Counterfactuals** offer actionable paths for rejected applicants
4. **PDP & Interactions** reveal non-linear relationships and feature synergies

#### Fairness Achievements
1. Eliminated age bias: Senior approval rate improved by 11%
2. Reduced income discrimination: Low-income approval increased by 8%
3. All fairness metrics now exceed 0.80 threshold (regulatory compliance)
4. Minimal accuracy trade-off (0.49%) for major fairness gain (27.8%)

#### Business Impact
- ‚úÖ Compliant with anti-discrimination regulations
- ‚úÖ Transparent decisions build customer trust
- ‚úÖ Expanded eligible customer base without increasing risk
- ‚úÖ Actionable feedback improves customer satisfaction
- ‚úÖ Defensible decisions reduce legal and reputational risk

---

## üè• Project 2: Medical Insurance Premium Prediction

### Dataset & Models

**Dataset Overview:**
- **File**: `Medicalpremium.csv`
- **Samples**: 986 | **Features**: 10 | **Target**: Premium Price ($15K-$40K)
- **Mean Premium**: $24,337 | **Std Dev**: $6,248

**Key Features:**
- Age, Diabetes, Blood Pressure Problems
- Chronic Diseases, Major Surgeries, Transplants
- Height, Weight, Known Allergies
- Family Cancer History

---

### Hyperparameter Tuning & Model Comparison

**Models Evaluated:**
1. Linear Regression
2. Ridge Regression (L2)
3. Lasso Regression (L1)
4. Decision Tree Regressor
5. Random Forest Regressor
6. Gradient Boosting Regressor

**Optimization Process:**
- GridSearchCV with 5-fold cross-validation
- Optimized for RMSE minimization
- Tested 80+ parameter combinations per model

![Hyperparameter Tuning](./regression_hyperparameter_tuning.png)

*GridSearchCV convergence and parameter optimization process*

---

#### Model Performance: Before vs After Tuning

![Model Comparison Before After](./regression_model_comparison_before_after.png)

*RMSE reduction through systematic hyperparameter optimization*

| Model | Base RMSE | Tuned RMSE | Improvement | Best Parameters |
|-------|-----------|------------|-------------|-----------------|
| Linear Regression | $3,845 | $3,496 | -9.1% | Default |
| Ridge Regression | $3,823 | $3,497 | -8.5% | alpha=1.0 |
| Lasso Regression | $3,867 | $3,501 | -9.5% | alpha=10.0 |
| Decision Tree | $3,234 | $2,746 | -15.1% | max_depth=10 |
| **Random Forest** | $2,456 | **$2,077** | **-15.4%** | See below |
| Gradient Boosting | $2,834 | $2,478 | -12.6% | learning_rate=0.1 |

**Best Model: Random Forest**

```python
# Optimal Hyperparameters
{
    'n_estimators': 200,
    'max_depth': 15,
    'min_samples_split': 5,
    'min_samples_leaf': 2,
    'max_features': 'sqrt',
    'bootstrap': True,
    'max_samples': 0.8
}
```

![Medical Model Comparison](./medical_model_comparison.png)

*Final RMSE, MAE, and R¬≤ comparison across all regression models*

---

#### Best Model Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **RMSE** | $2,077 | Average error: $2,077 per prediction |
| **MAE** | $959 | Median error: $959 per prediction |
| **R¬≤ Score** | 0.8988 | Explains 89.88% of premium variance |
| **MAPE** | 4.2% | 4.2% relative error on average |
| **CV RMSE** | $2,189 (¬±$267) | Robust across data splits |
| **Max Error** | $5,234 | Worst case prediction error |

**Model Quality:**
- ‚úÖ Excellent R¬≤ (>0.85)
- ‚úÖ Low RMSE relative to premium range
- ‚úÖ MAPE < 5% (industry standard)
- ‚úÖ Stable cross-validation performance

---

### Model Explainability - Regression

#### üåç Global Explainability

**Understanding model behavior across all customers**

---

**1. Feature Importance**

Identifies primary drivers of premium pricing.

![Feature Importance](./regression_feature_importance.png)

*Top 10 features ranked by predictive importance*

**Feature Rankings:**

| Rank | Feature | Importance | Interpretation |
|------|---------|------------|----------------|
| 1 | **Age** | 28% | Primary premium driver |
| 2 | **Major Surgeries** | 22% | Strong risk indicator |
| 3 | **Chronic Diseases** | 18% | Significant cost factor |
| 4 | **Blood Pressure** | 12% | Moderate risk |
| 5 | **Weight** | 8% | Health proxy |
| 6 | **Diabetes** | 6% | Moderate risk |
| 7 | **Height** | 3% | BMI calculation |
| 8 | **Allergies** | 1% | Minor factor |
| 9 | **Cancer History** | 1% | Family risk |
| 10 | **Transplants** | 1% | Rare but high cost |

**Business Insight:** Top 3 features (Age, Surgeries, Chronic Diseases) account for 68% of premium determination.

---

**2. Partial Dependence Plots (PDP)**

Shows individual feature effects on premium.

![PDP - Age](./regression_pdp_age.png)

*Premium increases non-linearly with age, accelerating after 50*

![PDP - Surgeries](./regression_pdp_surgeries.png)

*Each surgery adds approximately $3,500 to premium*

![PDP - Chronic Diseases](./regression_pdp_chronic.png)

*Chronic diseases add ~$6,200 to base premium*

![PDP - Weight](./regression_pdp_weight.png)

*Weight effect is non-linear with thresholds at 70kg and 90kg*

**Key Insights:**

| Feature | Effect | Premium Impact |
|---------|--------|----------------|
| **Age 18-45** | Linear | +$200/year |
| **Age 45-60** | Exponential | +$500/year |
| **Age 60+** | Steep | +$800/year |
| **Surgery #1** | Fixed | +$3,500 |
| **Surgery #2** | Fixed | +$3,500 |
| **Surgery #3+** | Increasing | +$4,000+ |
| **Chronic Disease** | Yes/No | +$6,200 |
| **Blood Pressure** | Yes/No | +$2,150 |
| **Diabetes** | Yes/No | +$1,840 |

**Non-linear Relationships:**
- Age effect accelerates after 50 (not constant)
- Multiple surgeries compound (not additive)
- Weight has threshold effects (70kg, 90kg inflection points)

---

**3. Global SHAP Summary**

SHAP values show feature importance AND directional impact.

![SHAP Summary Plot](./regression_shap_summary.png)

*SHAP summary: Feature importance with directional effects (red=high, blue=low)*

**Reading the Plot:**
- **Y-axis**: Features ranked by importance
- **X-axis**: SHAP value (impact on premium)
- **Color**: Feature value (red=high, blue=low)
- **Width**: Distribution of impact across customers

**Interpretation:**
- **Age**: Red dots (older) consistently push premiums right (higher)
- **Surgeries**: More surgeries (red) always increase premium
- **Chronic Diseases**: Red (has disease) ‚Üí strong positive impact
- **Wide distribution**: High individual variation in impacts

---

![SHAP Dependence - Age](./regression_shap_dependence_age.png)

*SHAP dependence plot: Exact premium impact per age value*

**Reading Dependence Plots:**
- **X-axis**: Age values (18-66)
- **Y-axis**: SHAP value ($ change from base)
- **Each dot**: One customer
- **Color**: Interaction with another feature (e.g., surgeries)

**Insights:**
- Non-linear relationship clearly visible
- Steep increase after age 50
- Scatter shows individual variation
- Color reveals interaction effects

---

#### üéØ Local Explainability

**Understanding predictions for specific customers**

---

**1. Single Prediction Explanation**

Detailed breakdown for an individual customer.

**Customer Profile #456:**
```
Age: 52 years
Diabetes: Yes (1)
Blood Pressure: Yes (1)
Chronic Diseases: Yes (1)
Major Surgeries: 2
Weight: 85 kg
Height: 170 cm (5'7")
Known Allergies: No (0)
Cancer History: No (0)
Transplants: No (0)
```

![Single Prediction](./regression_single_prediction.png)

*Actual vs Predicted premium with confidence interval*

**Prediction Details:**
- **Predicted Premium**: $31,450
- **Actual Premium**: $31,200
- **Prediction Error**: $250 (0.8% error)
- **95% Confidence Interval**: [$30,100 - $32,800]
- **Prediction Quality**: ‚úÖ Excellent (within 1% error)

---

**2. SHAP Waterfall Plot**

Visualizes step-by-step premium calculation from base to final.

![SHAP Waterfall](./regression_shap_waterfall.png)

*Waterfall showing feature-by-feature premium build-up for Customer #456*

**Premium Calculation Breakdown:**

```
Base Premium (Population Average):        $24,337

Positive Contributors (Increase Premium):
  + Age (52 years)                        +$4,250
  + Chronic Diseases (Yes)                +$6,180
  + Major Surgeries (2)                   +$3,920
  + Blood Pressure (Yes)                  +$2,150
  + Diabetes (Yes)                        +$1,840
  + Weight (85kg, above ideal)            +$920
                                          --------
  Subtotal Increases:                     +$19,260

Negative Contributors (Decrease Premium):
  - Cancer History (No)                   -$1,200
  - Transplants (No)                      -$447
  - Known Allergies (No)                  -$380
  - Height (170cm, healthy)               -$120
                                          --------
  Subtotal Decreases:                     -$2,147

Net Effect:                               +$7,113
                                          ========
Final Predicted Premium:                  $31,450
```

**Customer-Friendly Explanation:**

> *"Your premium of $31,450 is $7,
